{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff41e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde64506",
   "metadata": {},
   "source": [
    "# 상권 유형 예측 \n",
    "\n",
    "활용 데이터\n",
    "1. 지하철역 \n",
    "2. 임대주택 정보\n",
    "3. 주차장\n",
    "4. 상권 밀집도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca687be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv('../data/1-6.화성시_지하철역.csv')\n",
    "df2= pd.read_csv('../data/1-7.화성시_공영주차장.csv')\n",
    "df3=pd.read_csv('../data/1-12.공공주택임대_정보(화성시).csv')\n",
    "df4=pd.read_csv('../data/화성2022.csv') # 타겟값: 상권 유형을 예측해야 함\n",
    "gdf = gpd.read_file('../data/1-14.화성시_격자.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9098f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns= ['역이름','지하철노선이름','경도','위도']\n",
    "df2.columns = ['이름', '종류', '주차면수', '경도', '위도'] # 101\n",
    "df3.columns= ['블록코드','단지명','지원유형','세대수','주차면수','경도','위도']\n",
    "df4=df4.rename({'fac_density':'상권밀집도'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61126495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "일반상권    13391\n",
       "복합상권     9824\n",
       "주택상권     9401\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e76ead09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# NaN을 새로운 카테고리로 대체\n",
    "df4['cluster'] = df4['cluster'].fillna('미확인')\n",
    "\n",
    "# 수동 매핑 설정\n",
    "mapping = {'일반상권': 1, '주택상권': 2, '복합상권': 3}\n",
    "\n",
    "# cluster 열 매핑 적용 및 NaN 처리\n",
    "df4['cluster_encoded'] = df4['cluster'].map(mapping).fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5dcb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=df4[['gid','상권밀집도','cluster_encoded']]\n",
    "gdf2=target.merge(gdf)\n",
    "def calculate_centroid(geometry):\n",
    "    return geometry.centroid\n",
    "\n",
    "def calculate_distance(coord1, coord2):\n",
    "    return geodesic(coord1, coord2).meters\n",
    "\n",
    "gdf2['centroid'] = gdf2['geometry'].apply(calculate_centroid)\n",
    "gdf2['centroid_lon'] = gdf2['centroid'].apply(lambda x: x.x)\n",
    "gdf2['centroid_lat'] = gdf2['centroid'].apply(lambda x: x.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6570f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리 계산 함수\n",
    "def calculate_min_distance(df1_coords, target_coords):\n",
    "    distances = np.array([\n",
    "        geodesic(target_coords, station_coords).meters\n",
    "        for station_coords in df1_coords\n",
    "    ])\n",
    "    return distances.min()\n",
    "\n",
    "# df1의 지하철역 좌표 준비\n",
    "df1_coords = df1[['위도', '경도']].values\n",
    "\n",
    "# 각 격자 중심과 가장 가까운 지하철역의 거리 계산\n",
    "def calculate_distances(row):\n",
    "    target_coords = (row['centroid_lat'], row['centroid_lon'])\n",
    "    return calculate_min_distance(df1_coords, target_coords)\n",
    "\n",
    "gdf2['nearest_subway_distance'] = gdf2.apply(calculate_distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1605eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from math import radians\n",
    "\n",
    "# 주차장 좌표를 radians로 변환\n",
    "df2_coords_rad = np.radians(df2[['위도', '경도']].values)\n",
    "gdf2_coords_rad = np.radians(gdf2[['centroid_lat', 'centroid_lon']].values)\n",
    "\n",
    "def calculate_parking_stats_vectorized():\n",
    "    distances = haversine_distances(gdf2_coords_rad, df2_coords_rad) * 6371000  # 반지름 곱하기 지구 반경(미터)\n",
    "    nearest_distances = distances.min(axis=1)  # 가장 가까운 거리\n",
    "    return nearest_distances\n",
    "\n",
    "# 가장 가까운 주차장과의 거리\n",
    "gdf2['nearest_parking_distance'] = calculate_parking_stats_vectorized()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b11f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리와 세대수를 반영한 가중 평균 계산 함수\n",
    "def calculate_weighted_average(row, df3):\n",
    "    target_coords = (row['centroid_lat'], row['centroid_lon'])\n",
    "    df3['distance'] = df3.apply(\n",
    "        lambda x: geodesic(target_coords, (x['위도'], x['경도'])).meters, axis=1\n",
    "    )\n",
    "    # 거리의 역수와 세대수를 곱해 가중치 계산\n",
    "    df3['weight'] = df3['세대수'] / df3['distance']\n",
    "    weighted_sum = (df3['weight'] * df3['세대수']).sum()\n",
    "    total_weight = df3['weight'].sum()\n",
    "    return weighted_sum / total_weight if total_weight > 0 else 0\n",
    "\n",
    "# 격자별 계산\n",
    "gdf2['weighted_avg_households'] = gdf2.apply(lambda row: calculate_weighted_average(row, df3), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be0bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2.to_csv('../data/상권유형예측.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b901174",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2=pd.read_csv('../data/상권유형예측.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77e2303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   2.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.5s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.3s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=15, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=15, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   4.4s\n",
      "Random Forest Best Parameters: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     11635\n",
      "           1       0.97      0.98      0.97      3993\n",
      "           2       0.92      0.94      0.93      2830\n",
      "           3       0.95      0.91      0.93      2958\n",
      "\n",
      "    accuracy                           0.98     21416\n",
      "   macro avg       0.96      0.96      0.96     21416\n",
      "weighted avg       0.98      0.98      0.98     21416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비\n",
    "X = gdf2[['nearest_subway_distance', 'nearest_parking_distance', 'weighted_avg_households','상권밀집도']]\n",
    "y = gdf2['cluster_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest 하이퍼파라미터 튜닝\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [10,15],\n",
    "    'min_samples_split': [2,4],\n",
    "    'min_samples_leaf': [1,2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델로 예측 및 평가\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(\"Random Forest Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99382708",
   "metadata": {},
   "source": [
    "## 적절한 파라미터로 충분히 좋은 성능을 기록하는 것으로 보아 트리의 깊이를 더 깊게 할 필요가 없어 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69489e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
